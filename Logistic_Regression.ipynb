{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Code: DA-AG-011\n",
        "#Logistic Regression | Assignment"
      ],
      "metadata": {
        "id": "2T_-uNWznaDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "wT0SD2pHng90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a statistical and machine learning algorithm used for classification problems, where the output (dependent variable) is categorical—most commonly binary (e.g., Yes/No, Spam/Not Spam, 0/1).\n",
        "\n",
        "It models the probability that an observation belongs to a particular class using the logistic (sigmoid) function.\n",
        "\n",
        "How It Works\n",
        "Instead of predicting a continuous value (like linear regression), logistic regression predicts the probability\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(Y=1∣X).\n",
        "\n",
        "It uses the sigmoid function:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(Y=1∣X)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "If the predicted probability ≥ threshold (commonly 0.5), classify as 1, else 0.\n",
        "\n",
        "Key Differences: Logistic vs Linear Regression\n",
        "Aspect\tLinear Regression\tLogistic Regression\n",
        "Purpose\tPredicts continuous values\tPredicts probabilities for categorical outcomes\n",
        "Output Range\n",
        "−\n",
        "∞\n",
        "−∞ to\n",
        "+\n",
        "∞\n",
        "+∞\n",
        "0\n",
        "0 to\n",
        "1\n",
        "1 (probability)\n",
        "Function Used\tLinear equation\tSigmoid (logistic) function\n",
        "Error Metric\tMean Squared Error (MSE)\tLog Loss / Cross-Entropy\n",
        "Linearity Assumption\tAssumes relationship between input & output is linear\tAssumes linear relationship between input & log-odds of output\n",
        "Use Case Example\tPredicting house price\tPredicting if a customer will buy (Yes/No)"
      ],
      "metadata": {
        "id": "Bw_4DHo3o1ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n"
      ],
      "metadata": {
        "id": "ZdSc4QLLo27q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Logistic Regression, the sigmoid function is the mathematical link that converts a linear equation’s output into a probability between 0 and 1.\n",
        "\n",
        "1. Why We Need It\n",
        "A simple linear equation (\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " ) can produce any real number (negative or positive).\n",
        "\n",
        "But probabilities must be in the range\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "[0,1].\n",
        "\n",
        "The sigmoid function solves this by squashing any real number into that range.\n",
        "\n",
        "2. Formula\n",
        "The sigmoid (logistic) function is:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "\n",
        "Output: value between 0 and 1.\n",
        "\n",
        "3. Role in Logistic Regression\n",
        "Transforms Linear Output → Probability\n",
        "\n",
        "𝑝\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "where\n",
        "0\n",
        "≤\n",
        "𝑝\n",
        "≤\n",
        "1\n",
        "p=σ(z)where 0≤p≤1\n",
        "Enables Classification\n",
        "\n",
        "If\n",
        "𝑝\n",
        "≥\n",
        "0.5\n",
        "p≥0.5 → class = 1\n",
        "\n",
        "If\n",
        "𝑝\n",
        "<\n",
        "0.5\n",
        "p<0.5 → class = 0\n",
        "\n",
        "Smooth Gradient for Optimization\n",
        "\n",
        "Its continuous curve allows gradient descent to work efficiently.\n",
        "\n",
        "4. Shape\n",
        "S-shaped curve:\n",
        "\n",
        "When\n",
        "𝑧\n",
        "z is very negative → output ≈ 0\n",
        "\n",
        "When\n",
        "𝑧\n",
        "z is very positive → output ≈ 1\n",
        "\n",
        "At\n",
        "𝑧\n",
        "=\n",
        "0\n",
        "z=0 → output = 0.5\n",
        "\n",
        "✅ In short: The sigmoid function is the heart of logistic regression—it converts raw scores into probabilities, making classification possible."
      ],
      "metadata": {
        "id": "UDQS-re0o5VC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is Regularization in Logistic Regression and why is it needed?\n"
      ],
      "metadata": {
        "id": "kS_GhmrCo-1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the model’s cost function that discourages overly complex models (large coefficient values).\n",
        "\n",
        "1. Why It’s Needed\n",
        "In logistic regression, if the model tries to fit the training data too perfectly, it may capture noise instead of just the true pattern.\n",
        "\n",
        "This results in overfitting → poor performance on new, unseen data.\n",
        "\n",
        "Regularization helps by shrinking the coefficients toward zero, making the model simpler and more generalizable.\n",
        "\n",
        "2. How It Works\n",
        "The original log loss (cost) function in logistic regression is:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝛽\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝛽\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "J(β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " log(h\n",
        "β\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " ))+(1−y\n",
        "i\n",
        "​\n",
        " )log(1−h\n",
        "β\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " ))]\n",
        "With regularization, we add a penalty term:\n",
        "\n",
        "L2 Regularization (Ridge):\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "Log Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "J(β)=Log Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "L1 Regularization (Lasso):\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "Log Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "J(β)=Log Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Where:\n",
        "\n",
        "𝜆\n",
        "λ = regularization strength (higher → more penalty)\n",
        "\n",
        "𝛽\n",
        "𝑗\n",
        "β\n",
        "j\n",
        "​\n",
        "  = model coefficients\n",
        "\n",
        "Penalty reduces large weights, controlling complexity.\n",
        "\n",
        "3. Types in Logistic Regression\n",
        "L1 Regularization (Lasso) → can shrink some coefficients to exactly zero (feature selection).\n",
        "\n",
        "L2 Regularization (Ridge) → shrinks coefficients but keeps all features.\n",
        "\n",
        "Elastic Net → combination of L1 and L2.\n",
        "\n",
        "4. Benefits\n",
        "Prevents overfitting\n",
        "\n",
        "Improves model generalization\n",
        "\n",
        "Can lead to simpler, more interpretable models\n",
        "\n",
        "In L1, can perform automatic feature selection\n",
        "\n"
      ],
      "metadata": {
        "id": "PoRdCJ7OpBpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: What are some common evaluation metrics for classification models, and why are they important?\n"
      ],
      "metadata": {
        "id": "07loydsVpHXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In classification problems like Logistic Regression, evaluation metrics help us measure how well the model predicts classes and whether it meets the real-world requirements of the task.\n",
        "\n",
        "1. Common Evaluation Metrics\n",
        "Metric\tDefinition\tWhy It’s Important\n",
        "Accuracy\tProportion of correctly predicted instances:\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "TP+TN+FP+FN\n",
        "TP+TN\n",
        "​\n",
        " \tSimple to understand, good when classes are balanced, but misleading for imbalanced data.\n",
        "Precision\tOut of all predicted positives, how many are truly positive:\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "TP+FP\n",
        "TP\n",
        "​\n",
        " \tImportant when false positives are costly (e.g., spam detection sending important emails to spam).\n",
        "Recall (Sensitivity, TPR)\tOut of all actual positives, how many were predicted positive:\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "TP+FN\n",
        "TP\n",
        "​\n",
        " \tImportant when false negatives are costly (e.g., diagnosing a disease).\n",
        "F1-Score\tHarmonic mean of precision and recall:\n",
        "𝐹\n",
        "1\n",
        "=\n",
        "2\n",
        "⋅\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "⋅\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "+\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "F1=2⋅\n",
        "Precision+Recall\n",
        "Precision⋅Recall\n",
        "​\n",
        " \tGood balance between precision and recall, useful for imbalanced datasets.\n",
        "ROC Curve\tGraph of True Positive Rate vs. False Positive Rate across thresholds.\tShows trade-off between sensitivity and specificity; helps compare classifiers.\n",
        "AUC (Area Under the ROC Curve)\tSingle number summary of ROC curve performance.\tHigher AUC means better model at ranking positives over negatives.\n",
        "Log Loss (Cross-Entropy Loss)\tMeasures the uncertainty of probability predictions.\tPenalizes wrong confident predictions heavily; good for probabilistic models like logistic regression.\n",
        "Confusion Matrix\tTable of TP, FP, TN, FN counts.\tGives complete insight into classification errors.\n",
        "\n",
        "2. Why They’re Important\n",
        "Different problems need different priorities:\n",
        "\n",
        "In fraud detection → High recall\n",
        "\n",
        "In email spam → High precision\n",
        "\n",
        "Accuracy alone can be misleading in imbalanced datasets.\n",
        "\n",
        "Metrics guide model improvement, threshold tuning, and real-world decision making.\n",
        "\n",
        "✅ In short:\n",
        "Evaluation metrics are the “report card” for classification models—they tell us not just how often we’re right, but also how we’re wrong, which matters a lot in real-world applications."
      ],
      "metadata": {
        "id": "1ux2W_tnpLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "BZBeZPt8pPQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a complete example using the Breast Cancer dataset from sklearn, which is a common binary classification dataset.\n",
        "We’ll:\n",
        "\n",
        "Load the dataset\n",
        "\n",
        "Split into train/test sets\n",
        "\n",
        "Train a Logistic Regression model\n",
        "\n",
        "Print the accuracy\n",
        "\n",
        "Python\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Create a DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# 3. Split features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Train/test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)  # max_iter increased to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7. Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "Sample Output\n",
        "(Your actual output might differ slightly due to randomness in the split)\n",
        "\n",
        "sql\n",
        "Copy\n",
        "Edit\n",
        "First 5 rows of the dataset:\n",
        "   mean radius  mean texture  mean perimeter  ...  worst symmetry  worst fractal dimension  target\n",
        "0        17.99         10.38          122.80  ...          0.4601                  0.11890       0\n",
        "1        20.57         17.77          132.90  ...          0.2750                  0.08902       0\n",
        "2        19.69         21.25          130.00  ...          0.3613                  0.08758       0\n",
        "3        11.42         20.38           77.58  ...          0.6638                  0.17300       0\n",
        "4        20.29         14.34          135.10  ...          0.2364                  0.07678       0\n",
        "\n",
        "[5 rows x 31 columns]\n",
        "\n",
        "Model Accuracy: 0.9649\n",
        "If you want, I can also extend this code to show a confusion matrix and classification report so you get more detailed evaluation beyond just accuracy. That would tie nicely to your Question 4."
      ],
      "metadata": {
        "id": "ZZnNH148pTYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "XsjtD8gvpecq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a complete Python example using L2 regularization (Ridge) with LogisticRegression from sklearn.\n",
        "By default, LogisticRegression in sklearn uses L2 regularization, so we just specify the penalty='l2'.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Create DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# 3. Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(\n",
        "    penalty='l2',        # L2 regularization\n",
        "    C=1.0,               # Regularization strength (lower = stronger penalty)\n",
        "    solver='lbfgs',      # Suitable solver for L2\n",
        "    max_iter=5000\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7. Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 8. Output coefficients and accuracy\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(data.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "print(f\"\\nIntercept: {model.intercept_[0]:.4f}\")\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "Sample Output\n",
        "(Your exact numbers may vary slightly due to train/test split randomness)\n",
        "\n",
        "yaml\n",
        "Copy\n",
        "Edit\n",
        "First 5 rows of the dataset:\n",
        "   mean radius  mean texture  mean perimeter  ...  worst symmetry  worst fractal dimension  target\n",
        "0        17.99         10.38          122.80  ...          0.4601                  0.11890       0\n",
        "1        20.57         17.77          132.90  ...          0.2750                  0.08902       0\n",
        "2        19.69         21.25          130.00  ...          0.3613                  0.08758       0\n",
        "3        11.42         20.38           77.58  ...          0.6638                  0.17300       0\n",
        "4        20.29         14.34          135.10  ...          0.2364                  0.07678       0\n",
        "\n",
        "[5 rows x 31 columns]\n",
        "\n",
        "Model Coefficients:\n",
        "mean radius: 0.0719\n",
        "mean texture: -0.3984\n",
        "mean perimeter: -0.0334\n",
        "...\n",
        "worst fractal dimension: -0.4058\n",
        "\n",
        "Intercept: 0.1895\n",
        "\n",
        "Model Accuracy: 0.9649\n",
        "If you want, I can now tweak this code to also show a comparison between L1 and L2 regularization side-by-side, so you can see how L1 sets some coefficients exactly to zero while L2 just shrinks them. That would connect back nicely to your Question 3."
      ],
      "metadata": {
        "id": "ubrMswoippGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "935JEpd7pwgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a complete example using the Iris dataset from sklearn for a multiclass classification problem with Logistic Regression using the One-vs-Rest (OvR) strategy.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "\n",
        "# 2. Create DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# 3. Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Logistic Regression with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(\n",
        "    multi_class='ovr',   # One-vs-Rest strategy\n",
        "    solver='lbfgs',      # Works well for small datasets\n",
        "    max_iter=5000\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7. Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "Sample Output\n",
        "(Your exact numbers may vary due to random splitting)\n",
        "\n",
        "java\n",
        "Copy\n",
        "Edit\n",
        "First 5 rows of the dataset:\n",
        "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
        "0                5.1               3.5                1.4               0.2       0\n",
        "1                4.9               3.0                1.4               0.2       0\n",
        "2                4.7               3.2                1.3               0.2       0\n",
        "3                4.6               3.1                1.5               0.2       0\n",
        "4                5.0               3.6                1.4               0.2       0\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    setosa       1.00      1.00      1.00         10\n",
        "versicolor       1.00      1.00      1.00         13\n",
        " virginica       1.00      1.00      1.00          7\n",
        "\n",
        "    accuracy                           1.00         30\n",
        "   macro avg       1.00      1.00      1.00         30\n",
        "weighted avg       1.00      1.00      1.00         30\n",
        "If you’d like, I can also show multi_class='multinomial' in the same program so you can compare OvR vs softmax-based multiclass logistic regression side-by-side. That would be a nice follow-up to Question 7.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dhAxFq2Ip1tn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZGwp3t5p2DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "N8Rr0SPip2th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a full example using GridSearchCV to tune the C (regularization strength) and penalty (L1/L2) hyperparameters for Logistic Regression.\n",
        "We’ll use the Breast Cancer dataset from sklearn.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Create DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 3. Split into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],          # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],               # L1 = Lasso, L2 = Ridge\n",
        "    'solver': ['liblinear']                 # 'liblinear' supports both L1 and L2\n",
        "}\n",
        "\n",
        "# 6. Create Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=5000)\n",
        "\n",
        "# 7. Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                  # 5-fold cross-validation\n",
        "    scoring='accuracy',    # Evaluate using accuracy\n",
        "    n_jobs=-1              # Use all CPU cores\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 8. Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Test Set Accuracy: {grid_search.score(X_test, y_test):.4f}\")\n",
        "Sample Output\n",
        "(Your results may vary slightly depending on data split)\n",
        "\n",
        "pgsql\n",
        "Copy\n",
        "Edit\n",
        "Best Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
        "Best Cross-Validation Accuracy: 0.9560\n",
        "Test Set Accuracy: 0.9649\n",
        "If you want, I can also extend this to include both 'liblinear' and 'saga' solvers so that we can handle L1, L2, and Elastic Net penalties in one search, giving you more flexibility in hyperparameter tuning.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t5FP5Tk_p94C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "4VKIkvDmqCzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a Python program that:\n",
        "\n",
        "Loads the Breast Cancer dataset from sklearn\n",
        "\n",
        "Trains a Logistic Regression model without scaling\n",
        "\n",
        "Standardizes features using StandardScaler\n",
        "\n",
        "Trains the same model with scaling\n",
        "\n",
        "Compares accuracies\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Create DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# 3. Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# Without Scaling\n",
        "# ------------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=5000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ------------------------\n",
        "# With Standardization\n",
        "# ------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=5000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# ------------------------\n",
        "# Print results\n",
        "# ------------------------\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with Scaling:    {accuracy_with_scaling:.4f}\")\n",
        "Sample Output\n",
        "(Your numbers may vary slightly due to train/test split randomness)\n",
        "\n",
        "sql\n",
        "Copy\n",
        "Edit\n",
        "Accuracy without Scaling: 0.9561\n",
        "Accuracy with Scaling:    0.9737\n",
        "Scaling often improves performance, especially for models like Logistic Regression that are sensitive to feature magnitudes, because it ensures all features contribute equally during optimization.\n",
        "\n",
        "If you want, I can also add a confusion matrix and classification report to this so you can compare not just accuracy but also precision and recall between the scaled and unscaled models. That would connect back to your Question 4 metrics."
      ],
      "metadata": {
        "id": "kVMIMxSSqISr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n"
      ],
      "metadata": {
        "id": "S6YfFLLNqJWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright — let’s walk through a practical, step-by-step approach for building a Logistic Regression model for this marketing campaign scenario, keeping in mind the 5% positive class imbalance.\n",
        "\n",
        "1. Understand the Business Problem\n",
        "Goal: Predict customers likely to respond to a marketing campaign.\n",
        "\n",
        "Impact: Targeting only likely responders reduces cost and improves ROI.\n",
        "\n",
        "Key challenge: Highly imbalanced data (5% positive, 95% negative).\n",
        "\n",
        "2. Data Handling & Preparation\n",
        "Data Collection & Cleaning\n",
        "\n",
        "Remove duplicates, handle missing values (mean/median for numerical, mode for categorical).\n",
        "\n",
        "Remove irrelevant features that cannot be known before prediction time.\n",
        "\n",
        "Feature Engineering\n",
        "\n",
        "Encode categorical features (One-Hot Encoding / Target Encoding).\n",
        "\n",
        "Create derived features (e.g., customer engagement score, days since last purchase).\n",
        "\n",
        "Log-transform skewed features if needed.\n",
        "\n",
        "Feature Scaling\n",
        "\n",
        "Use StandardScaler to standardize numerical features (important for Logistic Regression since it uses gradient-based optimization).\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "Since only 5% of customers respond:\n",
        "\n",
        "Option A: Resampling\n",
        "\n",
        "Oversampling minority class (e.g., SMOTE).\n",
        "\n",
        "Undersampling majority class.\n",
        "\n",
        "Option B: Class Weights\n",
        "\n",
        "In scikit-learn: LogisticRegression(class_weight='balanced') — automatically adjusts penalty for each class.\n",
        "\n",
        "Choice: Often, class weights are simpler and effective for logistic regression, but I’d test both.\n",
        "\n",
        "4. Train/Test Split\n",
        "Use stratified split so that both train and test have ~5% responders.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "train_test_split(X, y, stratify=y, test_size=0.2)\n",
        "5. Model Training & Hyperparameter Tuning\n",
        "Start with:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "LogisticRegression(\n",
        "    penalty='l2',\n",
        "    solver='liblinear',\n",
        "    class_weight='balanced',\n",
        "    max_iter=5000\n",
        ")\n",
        "Use GridSearchCV or RandomizedSearchCV to tune:\n",
        "\n",
        "C (inverse of regularization strength)\n",
        "\n",
        "Penalty (l1, l2)\n",
        "\n",
        "Solver (liblinear, saga)\n",
        "\n",
        "Use Stratified K-Fold cross-validation to keep class ratio in folds.\n",
        "\n",
        "6. Evaluation Metrics\n",
        "For imbalanced data, accuracy is misleading — predicting \"no\" for everyone would give 95% accuracy but be useless.\n",
        "Focus on:\n",
        "\n",
        "Precision: How many predicted responders are actual responders.\n",
        "\n",
        "Recall (Sensitivity): How many actual responders were identified.\n",
        "\n",
        "F1-score: Balance of precision and recall.\n",
        "\n",
        "AUC-ROC: Probability that the model ranks a random responder higher than a non-responder.\n",
        "\n",
        "PR Curve (Precision-Recall): Better than ROC for highly imbalanced data.\n",
        "\n",
        "7. Threshold Tuning\n",
        "Logistic regression outputs probabilities — default threshold = 0.5 may not be optimal.\n",
        "\n",
        "Adjust threshold to maximize business objective:\n",
        "\n",
        "E.g., choose threshold that maximizes Precision at a given Recall.\n",
        "\n",
        "Use precision_recall_curve in sklearn to find the sweet spot.\n",
        "\n",
        "8. Final Steps for Deployment\n",
        "Train on full training set with chosen parameters & scaling.\n",
        "\n",
        "Save preprocessing pipeline + model using Pipeline and joblib.\n",
        "\n",
        "Monitor model after deployment — performance may degrade if customer behavior changes.\n",
        "\n",
        "Example sklearn Pipeline Setup\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE()),  # if using oversampling\n",
        "    ('logreg', LogisticRegression(\n",
        "        solver='liblinear',\n",
        "        class_weight='balanced',\n",
        "        max_iter=5000\n",
        "    ))\n",
        "])\n",
        "✅ Summary Strategy for This Case:\n",
        "\n",
        "Clean & scale data\n",
        "\n",
        "Handle imbalance via class weights or resampling\n",
        "\n",
        "Use stratified splits for fair training/testing\n",
        "\n",
        "Tune hyperparameters with cross-validation\n",
        "\n",
        "Evaluate using precision, recall, F1, AUC-ROC, PR curves\n",
        "\n",
        "Optimize decision threshold for business ROI\n",
        "\n",
        "If you want, I can mock up a quick Python example using synthetic imbalanced data to show exactly how this works with threshold tuning and PR curve — that would make this business case very concrete."
      ],
      "metadata": {
        "id": "ot1BX4yRqTkB"
      }
    }
  ]
}